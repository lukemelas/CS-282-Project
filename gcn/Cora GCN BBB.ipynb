{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_cora\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_cora()\n",
    "tmp = torch.load('../data/cora/preprocessed.pth')\n",
    "adj_i, adj_v, adj_s, features, labels, idx_train, idx_val, idx_test = tmp\n",
    "adj = torch.sparse.FloatTensor(adj_i, adj_v, adj_s)\n",
    "\n",
    "# Load pretrained\n",
    "pretrained = torch.load('save/model-val-0.8233.pth')\n",
    "pretrained_weights = torch.cat((pretrained['gcns.0.weight'].reshape(-1),\n",
    "                                pretrained['gcns.0.bias'].reshape(-1),\n",
    "                                pretrained['gcns.1.weight'].reshape(-1),\n",
    "                                pretrained['gcns.1.bias'].reshape(-1),),dim=0)\n",
    "Check: pretrained_weights[0:22928].reshape(1433, 16) - pretrained['gcns.0.weight']\n",
    "pretrained_init = torch.cat((pretrained_weights, -0.5 * torch.ones_like(pretrained_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Counter(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.values = []\n",
    "        self.total = 0.0\n",
    "        self.count = 0.0\n",
    "        \n",
    "    def push(self, x):\n",
    "        self.values.append(x)\n",
    "        self.total += x\n",
    "        self.count += 1\n",
    "        \n",
    "    def avg(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def rolling_avg(self, n):\n",
    "        return sum(self.values[-n:]) / n\n",
    "    \n",
    "def softplus(rho):\n",
    "    # convert from rho -> sigma\n",
    "    return torch.log(1 + torch.exp(rho))     \n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_weights = 0\n",
    "        self.use_bias = bias\n",
    "        \n",
    "        self.weight = None # Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.num_weights += in_features * out_features\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = None # Parameter(torch.FloatTensor(out_features))\n",
    "            self.num_weights += out_features\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "                    \n",
    "    def init_weights(self, weights):\n",
    "        self.weight = weights[:self.in_features * self.out_features].reshape(self.in_features, self.out_features)\n",
    "        if self.use_bias:\n",
    "            self.bias = weights[self.in_features * self.out_features:]\n",
    "            \n",
    "        # This could be a problem! \n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.layers = [self.gc1, self.gc2]\n",
    "        self.num_weights = sum(layer.num_weights for layer in self.layers)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def kernel(self, x):\n",
    "        ## Standard RBF kernel: x -> exp(-x*x)\n",
    "        return torch.nn.functional.relu(x) # torch.exp(-x*x)\n",
    "    \n",
    "    def init_params(self):\n",
    "        return torch.randn(self.num_weights * 2, requires_grad=True) # stored as [mu1, mu2, rho1, rho2]\n",
    "    \n",
    "    def init_weights(self, var_params):\n",
    "        ## Returns log_prob of weights, prior_prob of weights\n",
    "        w_dist = Normal(var_params[:self.num_weights], softplus(var_params[self.num_weights:]))\n",
    "        p_dist = Normal(torch.zeros(self.num_weights), torch.ones(self.num_weights) * 5)\n",
    "        weights = w_dist.rsample() # var_params[:self.num_weights] # w_dist.rsample()\n",
    "        \n",
    "        counter = 0\n",
    "        for layer in self.layers:\n",
    "            layer.init_weights(weights[counter:counter+layer.num_weights])\n",
    "            counter += layer.num_weights\n",
    "        \n",
    "        return w_dist.log_prob(weights), p_dist.log_prob(weights)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.kernel(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   :: Train LL: -1.838 | Val LL: -2.230 | Train Accuracy: 0.408 | Val Accuracy: 0.335 \n",
      "Train :: Epoch 1 | ELBO: -4.076 | LL: -2.216 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-51af56cb8a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# weight_decay=args.weight_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mbbb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-51af56cb8a87>\u001b[0m in \u001b[0;36mbbb\u001b[0;34m(adj, features, labels, idx_train, idx_val, model, params, epochs, samples, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m## Forward pass, compute log_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mmodel_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtotal_ll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel_ll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-77f05df1afed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-77f05df1afed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def accuracy(pred, true, idx):\n",
    "    ## Outputs accuracy on given index\n",
    "    assert(len(pred) == len(true))\n",
    "    return np.sum((pred == true)[idx]) / len(true[idx])\n",
    "\n",
    "def bbb(adj, features, labels, idx_train, idx_val, model, params, epochs=140, samples=10, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        if i % 10 == 0:\n",
    "            val(adj, features, labels, idx_train, idx_val, model, params)\n",
    "            model.train()\n",
    "        \n",
    "        ## Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(params.grad)\n",
    "        \n",
    "        total_ll = 0.0\n",
    "        total_wl = 0.0\n",
    "        total_wp = 0.0\n",
    "        \n",
    "        for j in range(samples):\n",
    "                        \n",
    "            ## Init weights\n",
    "            weight_ll, weight_prior = model.init_weights(params)\n",
    "            total_wl += weight_ll.sum() / model.num_weights\n",
    "            total_wp += weight_prior.sum() / model.num_weights\n",
    "\n",
    "            ## Forward pass, compute log_likelihood\n",
    "            pred = model(features, adj)\n",
    "            model_ll = - F.nll_loss(pred[idx_train], labels[idx_train])\n",
    "            total_ll += model_ll\n",
    "\n",
    "            ## Compute loss\n",
    "            ELBO = - model_ll - (weight_prior.sum() - weight_ll.sum()) / model.num_weights\n",
    "            # Why divide by weights twice? \n",
    "\n",
    "            ## Compute gradients\n",
    "            ELBO.backward()\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            total_ELBO = total_ll + total_wp - total_wl\n",
    "            print(\"Train :: Epoch {0:d} | ELBO: {1:.3f} | LL: {2:.3f} \".format(i + 1, total_ELBO / samples, total_ll / samples))\n",
    "        \n",
    "        ## Update parameters\n",
    "        # print(\"OLD\", params[:5])\n",
    "        optimizer.step()    \n",
    "        # print(\"NEW\", params[:5])\n",
    "        \n",
    "def val(adj, features, labels, idx_train, idx_val, model, params, samples=300, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    model.eval() # This could be a problem\n",
    "    \n",
    "    train_ll = 0.0\n",
    "    val_ll = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_acc = 0.0\n",
    "\n",
    "    for j in range(samples):\n",
    "        ## Forward pass\n",
    "        _ = model.init_weights(params)\n",
    "        pred = model(features, adj)\n",
    "        y_hat = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        ## collect log_likelihood statistics\n",
    "        train_ll += - F.nll_loss(pred[idx_train], labels[idx_train])\n",
    "        val_ll += - F.nll_loss(pred[idx_val], labels[idx_val])\n",
    "        \n",
    "        ## collect accuracy statistics\n",
    "        train_acc += accuracy(y_hat.numpy(), labels.numpy(), idx_train)\n",
    "        val_acc += accuracy(y_hat.detach().numpy(), labels.numpy(), idx_val)\n",
    "            \n",
    "    print(\"Val   :: Train LL: {0:.3f} | Val LL: {1:.3f} | Train Accuracy: {2:.3f} | Val Accuracy: {3:.3f} \".format(\n",
    "        train_ll / samples, val_ll / samples, train_acc / samples, val_acc / samples))\n",
    "        \n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5)\n",
    "\n",
    "var_params = model.init_params()\n",
    "var_params.data = pretrained_init.data.clone().cpu()\n",
    "# var_params.data.uniform_(0,0.01)\n",
    "optimizer = optim.Adam([var_params], lr=2e-2) # weight_decay=args.weight_decay\n",
    "\n",
    "bbb(adj, features, labels, idx_train, idx_val, model, var_params, samples=20)\n",
    "val(adj, features, labels, idx_train, idx_val, model, var_params, samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0 19  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ... 19  0  0]\n",
      " ...\n",
      " [ 0 19  0 ...  0  0  0]\n",
      " [ 0  0 19 ...  0  0  0]\n",
      " [ 0  0  0 ...  0 19  0]] [5 3 4 ... 1 2 5] [0.   0.05 0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65\n",
      " 0.7  0.75 0.8  0.85 0.9  0.95 1.   1.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE 0.02467028909052544\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.init_weights(var_params)\n",
    "probs = model(features, adj).detach().numpy()\n",
    "probs = np.exp(probs)\n",
    "# labels = labels.numpy()\n",
    "bins = np.arange(0, 1.1, 0.05)\n",
    "inds = np.digitize(probs, bins) - 1\n",
    "\n",
    "print(inds, labels, bins)\n",
    "\n",
    "def ECE(inds, labels, bins):\n",
    "    ## Produce Plot\n",
    "    labels_onehot = torch.zeros(7, len(labels)).scatter_(0,torch.LongTensor([labels]),1).numpy().T\n",
    "    bin_avgs = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    ece = 0\n",
    "    true_probs = []\n",
    "    for idx, b in enumerate(bins):\n",
    "        num = np.sum(inds == idx)\n",
    "        # print(labels_onehot, inds == idx, 'value', labels_onehot[inds == idx])\n",
    "        correct = np.sum(labels_onehot[inds == idx])\n",
    "        true_probs.append(correct/num)\n",
    "        if num > 0:\n",
    "            ece += num / (7 * len(labels)) * np.abs(bin_avgs[idx] - correct/num)     \n",
    "            # print(num / (7 * len(labels)) * np.abs(bin_avgs[idx] - correct/num))\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.bar(bin_avgs, true_probs[1:], width=bins[1]-bins[0]-0.01, color='blue', alpha=0.5) #true\n",
    "    plt.bar(bin_avgs, bin_avgs, width=bins[1]-bins[0]-0.01, color='orange', alpha=0.5) #ideal\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ECE\", ece)\n",
    "    \n",
    "ECE(inds, labels, bins) #whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE 0.0218357142857143\n"
     ]
    }
   ],
   "source": [
    "ECE(inds[idx_test], labels[idx_test], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(adj, features, labels, idx_train, idx_val, idx_test, model, params, samples=1000, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    train_ll = 0.0\n",
    "    val_ll = 0.0\n",
    "    test_ll = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    for j in range(samples):\n",
    "        ## Forward pass\n",
    "        _ = model.init_weights(params)\n",
    "        pred = model(features, adj)\n",
    "        y_hat = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        ## collect log_likelihood statistics\n",
    "        train_ll += - F.nll_loss(pred[idx_train], labels[idx_train])\n",
    "        val_ll += - F.nll_loss(pred[idx_val], labels[idx_val])\n",
    "        test_ll += - F.nll_loss(pred[idx_test], labels[idx_test])\n",
    "        \n",
    "        ## collect accuracy statistics\n",
    "        train_acc += accuracy(y_hat.numpy(), labels.numpy(), idx_train)\n",
    "        val_acc += accuracy(y_hat.numpy(), labels.numpy(), idx_val)\n",
    "        test_acc += accuracy(y_hat.numpy(), labels.numpy(), idx_test)\n",
    "            \n",
    "            \n",
    "    print(\"Test :: Train LL: {0:.3f} | Val LL: {1:.3f} | Test LL: {2:.3f} \".format(\n",
    "        train_ll / samples, val_ll / samples, test_ll / samples))\n",
    "    print(\"Test :: Train Acc: {0:.3f} | Val Acc: {1:.3f} | Test Acc: {2:.3f} \".format(\n",
    "        train_acc / samples, val_acc / samples, test_acc / samples))\n",
    "    \n",
    "test(adj, features, labels, idx_train, idx_val, idx_test, model, var_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = None\n",
    "\n",
    "for j in range(1000):\n",
    "    ## Forward pass\n",
    "    _ = model.init_weights(var_params)\n",
    "    if pred is not None:\n",
    "        pred += torch.exp(model(features, adj))\n",
    "    else:\n",
    "        pred = torch.exp(model(features, adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pred[idx_test]\n",
    "labels = labels[idx_test]\n",
    "guess = torch.argmax(pred, dim=1)\n",
    "correct = guess == labels\n",
    "wrong = guess != labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3603, 1.2366, 0.7163, 0.9059, 1.6553, 0.8282, 1.2415, 0.4502, 1.6636,\n",
       "        1.8381, 0.3000, 1.0848, 0.6153, 0.2018, 1.0080, 0.3347, 0.9520, 1.4775,\n",
       "        1.1042, 0.1209, 1.6844, 1.4668, 1.0457, 1.5584, 1.6741, 1.0121, 0.4431,\n",
       "        0.4373, 1.5516, 1.6314, 1.3661, 1.2409, 0.4992, 0.3843, 1.0915, 0.3969,\n",
       "        0.2143, 0.9615, 0.9302, 0.3421, 1.2179, 0.3613, 0.7717, 1.4395, 0.0963,\n",
       "        1.2443, 1.4173, 1.5498, 0.3102, 0.3460, 1.3068, 1.6502, 0.0806, 0.3665,\n",
       "        0.1731, 0.8521, 0.6557, 1.1187, 0.7254, 0.4201, 1.5736, 0.2589, 0.5086,\n",
       "        1.2704, 0.3669, 0.1868, 0.2815, 0.5458, 1.6864, 0.5012, 0.1060, 0.2724,\n",
       "        0.6947, 0.7801, 0.8286, 0.3371, 0.0677, 0.5936, 0.8561, 0.4497, 0.5007,\n",
       "        0.3303, 1.7694, 0.3361, 0.8834, 1.2275, 0.5014, 1.3169, 0.7456, 0.7288,\n",
       "        1.6292, 0.5110, 0.5746, 0.9860, 0.7626, 0.3631, 1.5574, 1.0405, 1.3766,\n",
       "        0.7371, 0.2833, 0.5042, 1.3460, 1.1541, 0.9515, 0.3335, 1.1740, 1.4945,\n",
       "        0.2300, 1.1782, 0.4101, 0.3760, 0.6821, 0.4003, 0.8352, 0.3392, 0.9612,\n",
       "        1.7773, 1.5091, 1.0632, 0.8441, 0.2431, 0.4011, 0.6804, 1.2610, 1.5169,\n",
       "        0.8398, 1.6499, 0.9030, 0.8156, 1.0684, 1.6136, 1.3764, 0.4279, 1.4137,\n",
       "        0.7551, 0.6777, 0.5879, 1.2453, 0.8937, 0.2231, 1.1222, 0.6410, 0.2800,\n",
       "        0.3162, 0.2814, 1.3228, 0.4251, 1.7921, 0.7785, 1.0456, 0.9553, 0.4497,\n",
       "        0.5352, 0.7557, 0.5142, 1.2770, 1.7480, 1.2045, 1.0959, 0.8914, 0.7055,\n",
       "        1.1087, 1.4953, 0.4902, 0.7137, 0.6066, 0.2833, 0.9095, 1.8282, 1.5721,\n",
       "        0.3257, 0.3433, 1.6105, 0.6235, 0.8253, 0.2811, 0.6427, 1.5516, 0.3612,\n",
       "        0.2760, 1.4709, 1.4774, 1.3968, 1.0520, 0.7201, 1.3966, 0.1089, 0.5595,\n",
       "        1.7634, 1.7298, 0.8001, 1.3972, 1.3860, 0.6069, 1.2350, 0.7217, 0.6551,\n",
       "        1.1527, 0.8178, 1.3699, 1.5340, 1.6553, 1.0593, 0.9565, 0.5135, 1.5908,\n",
       "        1.5050, 0.5225, 1.6314, 0.8084, 1.6761, 1.4711, 0.4694, 1.4074, 0.4186,\n",
       "        1.2226, 1.8217, 0.8011, 1.4098, 1.3871, 0.9504, 1.0282, 0.6802, 0.3919,\n",
       "        0.1170, 0.4994, 1.1235, 0.9415, 0.2452, 0.3067, 0.1059, 0.3288, 0.8763,\n",
       "        1.1756, 0.9465, 0.5275, 0.3992, 0.2689, 0.8240, 1.7915, 0.3404, 0.2363,\n",
       "        0.6504, 0.6978, 0.0487, 0.5974, 0.1788, 0.2500, 0.2520, 1.6576, 0.0647,\n",
       "        0.7770, 1.1215, 1.7085, 0.7955, 1.8080, 1.0449, 0.8785, 0.0666, 0.8709,\n",
       "        0.3941, 1.1423, 0.6895, 1.5298, 1.2441, 1.4380, 0.7893, 0.6152, 1.3581,\n",
       "        0.2992, 0.0869, 0.1303, 0.8672, 0.3357, 0.7409, 0.2110, 0.6925, 0.3766,\n",
       "        0.8789, 1.2154, 1.0543, 1.8017, 1.7476, 0.8345, 1.7430, 0.6936, 0.8089,\n",
       "        0.7411, 1.4353, 1.5148, 0.6310, 1.2178, 0.7722, 1.6686, 1.5604, 1.7808,\n",
       "        1.5885, 0.8947, 0.9812, 1.1905, 1.0733, 1.0860, 0.7286, 1.0470, 0.5327,\n",
       "        0.7450, 1.0905, 0.1713, 0.5500, 1.4255, 1.6296, 0.5357, 1.1572, 0.4603,\n",
       "        1.4625, 0.9365, 0.5375, 0.7152, 0.6850, 0.9429, 1.4260, 1.5997, 1.0863,\n",
       "        1.2178, 1.6647, 1.3070, 0.4872, 1.3835, 0.6242, 0.6175, 1.7299, 0.4625,\n",
       "        1.4531, 0.2483, 1.3578, 1.5985, 1.3237, 1.8220, 0.0736, 0.9530, 0.3009,\n",
       "        0.6242, 1.5472, 0.7407, 1.1948, 0.9621, 0.7727, 1.1085, 1.4513, 1.4398,\n",
       "        1.7740, 1.3139, 1.5174, 1.7570, 0.9885, 0.8885, 0.7292, 0.8972, 1.6754,\n",
       "        1.1980, 1.7221, 1.7518, 0.2738, 0.8271, 1.0100, 0.3122, 0.3783, 1.2501,\n",
       "        1.0203, 1.3835, 0.7299, 0.2535, 1.0985, 0.1296, 0.4645, 1.3995, 1.5895,\n",
       "        1.6485, 1.4229, 1.4625, 0.6870, 0.4445, 0.6800, 0.8651, 1.7264, 1.6384,\n",
       "        1.1311, 0.2852, 0.1051, 0.3122, 1.1987, 0.0863, 1.4276, 0.7794, 1.6915,\n",
       "        0.3038, 1.2107, 0.4930, 0.6987, 1.5634, 1.5506, 1.6050, 1.7393, 0.0631,\n",
       "        1.2406, 1.5549, 0.6880, 1.6231, 0.4872, 1.5432, 1.6611, 1.0600, 0.7572,\n",
       "        1.2049, 1.7077, 0.5350, 1.1301, 0.7151, 0.9983, 1.0985, 0.6859, 0.5833,\n",
       "        0.5550, 1.2043, 1.8016, 1.7182, 1.6956, 1.2637, 1.3677, 0.3363, 0.6507,\n",
       "        0.9752, 0.7272, 0.9004, 1.2464, 1.7513, 1.5139, 1.2457, 1.0954, 1.0802,\n",
       "        1.8244, 0.9157, 1.2304, 1.5849, 0.5943, 0.3769, 1.7380, 1.2671, 1.4319,\n",
       "        0.1631, 1.5246, 1.0772, 1.8096, 0.5949, 0.9841, 1.6038, 1.1811, 1.4586,\n",
       "        1.3237, 1.6867, 1.5806, 0.5626, 1.4339, 1.5864, 1.6381, 1.5352, 1.1288,\n",
       "        1.0401, 0.9204, 1.4531, 0.3614, 1.8096, 1.0587, 1.3643, 0.9019, 1.2064,\n",
       "        1.3627, 0.5992, 0.8010, 0.6684, 1.6401, 1.3483, 0.8855, 1.7078, 1.4105,\n",
       "        1.2200, 0.7232, 1.7570, 0.8347, 0.4853, 1.4037, 0.9878, 1.1952, 1.3954,\n",
       "        1.4488, 0.7359, 1.8259, 0.1557, 1.5047, 0.6503, 1.1845, 0.9097, 0.4615,\n",
       "        0.6058, 0.7502, 1.4325, 1.0757, 1.5824, 1.5502, 1.5762, 1.5829, 0.5280,\n",
       "        1.1351, 0.2912, 0.9805, 0.7551, 1.4022, 1.5224, 1.6622, 1.6357, 1.1091,\n",
       "        1.6531, 1.6038, 1.5116, 1.1880, 1.0754, 1.1358, 0.3164, 0.9907, 1.3155,\n",
       "        0.6983, 0.8508, 1.7203, 1.6754, 1.5083, 1.6690, 0.3325, 0.8175, 1.1899,\n",
       "        1.4470, 1.3669, 0.4229, 0.9802, 0.6031, 0.6639, 0.5056, 0.4830, 1.7078,\n",
       "        1.7695, 0.5868, 0.6339, 0.3618, 1.2821, 1.6345, 1.4034, 0.4194, 1.6835,\n",
       "        0.6628, 1.6510, 1.1003, 1.3492, 0.2469, 1.1447, 0.3997, 0.9527, 0.5108,\n",
       "        0.1914, 0.4218, 0.7591, 0.8718, 1.7724, 0.2606, 1.7227, 1.3201, 1.3628,\n",
       "        1.2517, 1.3297, 1.2517, 0.8859, 0.5113, 1.4702, 1.5092, 1.4587, 1.3105,\n",
       "        1.7810, 0.8584, 1.7224, 1.5636, 1.4941, 1.4757, 0.5330, 0.7940, 0.1756,\n",
       "        0.2679, 1.0006, 1.2487, 1.2976, 1.1876, 1.1438, 1.0583, 1.5203, 1.0570,\n",
       "        0.8111, 1.6282, 0.7936, 0.9347, 1.7824, 1.7341, 1.6604, 0.8125, 1.5278,\n",
       "        1.4657, 0.8596, 1.5248, 0.9015, 0.2754, 1.6720, 1.2391, 0.9477, 1.6359,\n",
       "        0.5471, 0.8397, 0.7346, 1.1789, 0.5170, 0.5144, 1.0386, 0.5859, 0.4941,\n",
       "        1.1878, 1.4537, 1.3856, 1.6980, 1.5353, 1.6482, 1.0769, 1.2550, 0.5633,\n",
       "        0.9995, 1.6337, 0.4272, 1.6943, 1.1913, 0.6054, 0.6532, 1.4937, 0.1212,\n",
       "        0.3612, 0.3530, 0.7947, 1.7617, 1.3393, 0.5250, 0.9327, 1.7581, 1.1351,\n",
       "        0.2293, 1.0866, 1.3343, 1.8718, 1.6315, 0.6735, 1.3758, 1.4563, 1.6808,\n",
       "        1.8718, 1.6031, 1.2537, 1.5349, 1.5256, 0.9614, 1.1678, 1.5940, 1.6372,\n",
       "        1.7733, 1.2243, 1.5938, 1.3689, 0.7032, 1.7517, 0.7913, 1.6920, 0.8589,\n",
       "        0.5562, 0.6997, 1.4389, 1.5371, 1.0843, 1.3771, 0.8109, 1.6605, 1.0445,\n",
       "        1.4990, 0.7445, 0.8686, 0.8522, 1.2929, 1.4753, 1.3878, 0.2675, 0.7083,\n",
       "        1.3397, 0.8726, 0.7934, 0.8569, 1.5655, 1.6854, 1.2027, 0.7452, 0.5904,\n",
       "        0.5109, 0.6918, 0.7152, 1.3487, 1.0425, 1.2642, 1.6440, 1.2818, 1.0672,\n",
       "        1.5472, 1.7325, 0.7399, 1.5046, 1.2077, 1.4136, 1.5516, 1.7478, 1.5264,\n",
       "        1.1795, 1.6463, 1.6455, 1.6712, 0.6924, 1.2642, 1.7785, 1.7538, 1.0892,\n",
       "        1.5787, 1.1906, 0.1372, 0.0936, 1.0954, 0.3899, 1.0003, 0.2429, 0.4960,\n",
       "        0.9800, 1.2509, 1.2520, 0.4000, 0.4000, 0.3560, 1.5387, 0.3138, 0.2350,\n",
       "        1.2113, 0.6460, 0.3248, 1.4265, 1.2803, 1.6031, 1.7555, 1.5084, 1.4204,\n",
       "        0.5115, 1.5452, 1.4785, 1.3938, 1.5454, 1.5325, 1.8599, 0.8503, 1.5178,\n",
       "        1.2733, 1.2367, 1.7146, 1.2871, 0.5323, 1.3816, 1.2587, 0.8465, 1.2697,\n",
       "        0.7640, 1.0556, 0.5727, 1.2146, 0.8886, 0.5304, 1.7199, 1.5265, 1.5223,\n",
       "        1.6364, 0.9682, 1.7543, 1.6384, 0.7212, 1.7870, 0.8245, 1.4033, 0.4832,\n",
       "        1.6050, 1.2449, 1.0397, 1.2883, 0.8691, 0.9232, 1.5905, 1.7385, 1.2990,\n",
       "        0.9445, 1.6590, 1.2991, 0.4347, 1.2922, 1.0794, 0.5352, 1.4050, 1.0950,\n",
       "        1.7429, 0.3083, 0.7065, 1.5958, 0.2871, 0.3053, 1.5282, 0.1727, 1.6409,\n",
       "        1.6654, 1.5348, 1.4425, 1.7181, 1.6653, 1.6678, 0.9217, 1.4600, 1.1455,\n",
       "        1.6303, 1.0089, 1.6659, 1.0902, 0.8887, 0.2753, 0.4832, 0.5210, 0.6248,\n",
       "        0.8492, 0.1289, 0.6716, 1.6440, 1.7617, 1.7614, 1.3783, 1.5354, 1.0071,\n",
       "        0.7324, 1.3192, 1.5741, 0.8944, 1.7290, 1.2998, 1.4035, 0.8898, 0.5350,\n",
       "        0.5116, 0.5945, 1.1576, 0.8442, 0.7074, 1.7642, 1.3334, 1.7635, 0.2094,\n",
       "        1.4807, 1.1810, 1.5482, 0.3296, 0.7824, 0.2008, 0.8983, 0.8249, 0.2728,\n",
       "        0.5734, 1.3056, 1.5591, 1.7962, 1.1763, 1.1906, 1.1655, 0.8401, 1.7429,\n",
       "        1.7429, 1.5721, 0.9473, 1.4069, 1.5628, 1.7407, 1.2671, 1.5214, 0.8280,\n",
       "        1.3233, 1.1095, 1.1786, 1.5764, 1.0459, 0.8587, 1.3998, 1.6905, 1.6132,\n",
       "        1.2789, 0.9244, 1.0327, 1.6866, 1.0922, 0.7942, 1.7986, 0.7067, 1.4814,\n",
       "        0.5704, 1.1512, 0.7027, 1.4898, 1.3711, 1.4726, 1.4251, 1.0771, 1.0802,\n",
       "        1.1100, 1.4514, 1.1425, 1.2193, 1.5285, 1.5589, 1.0126, 1.3545, 0.7684,\n",
       "        1.6915, 1.0395, 0.6673, 0.8651, 0.3249, 1.7980, 1.5540, 1.3317, 1.6026,\n",
       "        1.3117, 1.4507, 1.5332, 1.1511, 1.4000, 0.2420, 0.8451, 1.6249, 0.9201,\n",
       "        1.7131, 0.8580, 0.5558, 1.4104, 1.8352, 0.2817, 0.9949, 0.8512, 1.5441,\n",
       "        1.2307, 0.4217, 0.8950, 1.0992, 1.2162, 1.2065, 0.5031, 0.4428, 1.1264,\n",
       "        1.6938, 1.2459, 1.0904, 1.0980, 1.4564, 0.6027, 1.1216, 1.2892, 1.7024,\n",
       "        1.5682, 1.1107, 0.7456, 0.7227, 1.0543, 1.5642, 1.8504, 0.2059, 1.5762,\n",
       "        0.3791, 1.6914, 1.0456, 0.3427, 1.5847, 1.7270, 0.9823, 0.9942, 1.7209,\n",
       "        0.6190], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy = - torch.sum(pred / 1000 * torch.log(pred / 1000), dim=1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  1.,  0.,  0.,  4.,  2.,  0.,  2.,  7.,  4., 11.,  7., 10.,\n",
       "         6., 18., 16., 20., 23., 25., 17.]),\n",
       " array([0.2469458 , 0.32759136, 0.40823692, 0.48888248, 0.56952804,\n",
       "        0.6501736 , 0.73081917, 0.8114647 , 0.8921103 , 0.97275585,\n",
       "        1.0534015 , 1.134047  , 1.2146926 , 1.2953382 , 1.3759837 ,\n",
       "        1.4566293 , 1.5372748 , 1.6179204 , 1.698566  , 1.7792115 ,\n",
       "        1.8598571 ], dtype=float32),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADsVJREFUeJzt3W2MpWddx/Hvz7ZErQS67jBuKONismAKodBMmgoNAdaa\nBYpbE7MpUbMhm0w02mA1mtUXqO/6SsSn6KagY+SpAWs3BNBlgRDtA2yxQKGF1mYrbfahlCIUjaT1\n74u5V6bDTM8952HOmWu/n2Ry7sc5/7lz7W+vc537IVWFJGn7+6FpFyBJGg8DXZIaYaBLUiMMdElq\nhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIC7fyzXbu3Fm7d+/eyreUpG3v7rvv/kZVzQ3abksDfffu\n3Zw4cWIr31KStr0kD/fZziEXSWqEgS5JjTDQJakRBrokNcJAl6RG9Ar0JM9P8qEk9ye5L8nPJNmR\n5FiSB7rXSyZdrCRpY3176O8CPl5VPw1cDtwHHAaOV9Ue4Hg3L0makoGBnuR5wGuBdwNU1feq6lvA\nfmC522wZuG5SRUqSBuvTQ38x8BjwN0n+LcnNSS4G5qvqVLfNaWB+UkVKkgbrc6XohcAVwA1VdVeS\nd7FmeKWqKsm6T5tOsgQsASwsLIxYrraDdx772tD73njNS8ZYiXR+6dNDfwR4pKru6uY/xErAn0my\nC6B7PbvezlV1pKoWq2pxbm7grQgkSUMaGOhVdRr4epKXdov2Al8BjgIHu2UHgdsmUqEkqZe+N+e6\nAXhvkucADwFvY+U/g1uSHAIeBg5MpkRJUh+9Ar2q7gEW11m1d7zlSJKG5ZWiktQIA12SGmGgS1Ij\nDHRJaoSBLkmN2NJnikqDeJWpNDx76JLUCANdkhphoEtSIxxDl7Tl/K5kMuyhS1IjDHRJaoSBLkmN\nMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgD\nXZIa0esBF0lOAt8BngaeqqrFJDuADwK7gZPAgap6YjJlSrPNBzZoFmymh/76qnplVS1284eB41W1\nBzjezUuSpmSUIZf9wHI3vQxcN3o5kqRh9X2maAGfSPI08NdVdQSYr6pT3frTwPx6OyZZApYAFhYW\nRixX0moO9Wi1voF+dVU9muQFwLEk969eWVWVpNbbsQv/IwCLi4vrbiNJGl2vIZeqerR7PQvcClwJ\nnEmyC6B7PTupIiVJgw0M9CQXJ3nuuWng54B7gaPAwW6zg8BtkypSkjRYnyGXeeDWJOe2f19VfTzJ\n54BbkhwCHgYOTK5MSdIgAwO9qh4CLl9n+ePA3kkUJUnaPK8UlaRGGOiS1AgDXZIaYaBLUiMMdElq\nRN8rRbXNeEm4dP6xhy5JjTDQJakRDrlIUzbK8Nj5yOHEjdlDl6RGGOiS1AiHXCbMj4eStoo9dElq\nhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIT1tUMzxFVOc7e+iS1AgDXZIaYaBLUiMMdElqhIEuSY0w\n0CWpEb1PW0xyAXACeLSqrk2yA/ggsBs4CRyoqicmUeS0+QACSdvBZnrobwfuWzV/GDheVXuA4928\nJGlKegV6kkuBNwM3r1q8H1juppeB68ZbmiRpM/r20P8E+F3gf1ctm6+qU930aWB+nIVJkjZn4Bh6\nkmuBs1V1d5LXrbdNVVWS2mD/JWAJYGFhYYRSpck5H78nOR//5tb16aG/Bvj5JCeBDwBvSPL3wJkk\nuwC617Pr7VxVR6pqsaoW5+bmxlS2JGmtgYFeVb9XVZdW1W7geuCTVfXLwFHgYLfZQeC2iVUpSRpo\nlLst3gTckuQQ8DBwYDwl6Rw/EkvajE0FelV9Gvh0N/04sHf8JUmShuGVopLUCANdkhphoEtSIwx0\nSWqEgS5JjfAh0foBni6pVrX+IHF76JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RG\nGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasTA\nQE/yw0k+m+QLSb6c5I+65TuSHEvyQPd6yeTLlSRtpE8P/X+AN1TV5cArgX1JrgIOA8erag9wvJuX\nJE3JwECvFU92sxd1PwXsB5a75cvAdROpUJLUS68x9CQXJLkHOAscq6q7gPmqOtVtchqY32DfpSQn\nkpx47LHHxlK0JOkH9Qr0qnq6ql4JXApcmeTla9YXK7329fY9UlWLVbU4Nzc3csGSpPVt6iyXqvoW\n8ClgH3AmyS6A7vXs+MuTJPXV5yyXuSTP76Z/BLgGuB84ChzsNjsI3DapIiVJg13YY5tdwHKSC1j5\nD+CWqvpIkjuAW5IcAh4GDkywTknSAAMDvaq+CLxqneWPA3snUZQkafO8UlSSGmGgS1IjDHRJaoSB\nLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5Jjehzt0VJaspV3/2zIfYaZp+t\nZQ9dkhphoEtSIwx0SWqEgS5JjTDQJakRnuUiadsa7myVdtlDl6RGGOiS1AgDXZIasW3G0N957GtD\n73vjNS8ZYyWSNJvsoUtSIwx0SWrEwEBP8qIkn0rylSRfTvL2bvmOJMeSPNC9XjL5ciVJG+nTQ38K\n+O2qugy4Cvj1JJcBh4HjVbUHON7NS5KmZGCgV9Wpqvp8N/0d4D7ghcB+YLnbbBm4blJFSpIG29QY\nepLdwKuAu4D5qjrVrToNzI+1MknSpvQ+bTHJjwEfBn6zqr6d5P/XVVUlqQ32WwKWABYWFkarVlKz\nvIx/dL166EkuYiXM31tV/9AtPpNkV7d+F3B2vX2r6khVLVbV4tzc3DhqliSto89ZLgHeDdxXVX+8\natVR4GA3fRC4bfzlSZL66jPk8hrgV4AvJbmnW/b7wE3ALUkOAQ8DByZToiSpj4GBXlX/AmSD1XvH\nW44kaVheKSpJjTDQJakR2+Zui6MY5U6N0vlsmFMJ77z4hglUoj7soUtSIwx0SWrEeTHkIkkj++If\nDrffK4bcbwj20CWpEQa6JDXCQJekRjiGLmmsvGvi9NhDl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEu\nSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1YuADLpK8B7gW\nOFtVL++W7QA+COwGTgIHquqJyZUpzbZhHupw58U3TKASnc/69ND/Fti3Ztlh4HhV7QGOd/OSpCka\nGOhV9Rngm2sW7weWu+ll4Lox1yVJ2qRhnyk6X1WnuunTwPxGGyZZApYAFhYWhnw7Sec4vKONjPyl\naFUVUM+y/khVLVbV4tzc3KhvJ0nawLCBfibJLoDu9ez4SpIkDWPYQD8KHOymDwK3jaccSdKwBgZ6\nkvcDdwAvTfJIkkPATcA1SR4AfrablyRN0cAvRavqrRus2jvmWiRJI/BKUUlqxLCnLUrSeeWOhx4f\nar87z3wNgBuveck4y1mXPXRJaoSBLkmNMNAlqRGOoatZs36J/DD1Sc/GHrokNcJAl6RGOOQinQcc\n3jk/2EOXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjfC0xU0Y9tSvWX9Ab6t/1zA8vU/bmT10SWqE\ngS5JjTDQJakRBrokNcJAl6RGGOiS1AhPW9TQtvIBEp5OKA1mD12SGmGgS1IjRhpySbIPeBdwAXBz\nVd00lqoaM+vPtpTUhqF76EkuAP4CeCNwGfDWJJeNqzBJ0uaMMuRyJfBgVT1UVd8DPgDsH09ZkqTN\nGiXQXwh8fdX8I90ySdIUTPy0xSRLwFI3+2SSr26w6U7gG5OuZwy2qM4/H/UXzOjxfMbfNaM1/gDr\nHJ/tUCOMtc6VNv9bo/2Sn+yz0SiB/ijwolXzl3bLnqGqjgBHBv2yJCeqanGEeraEdY7PdqgRrHOc\ntkONsH3qXGuUIZfPAXuSvDjJc4DrgaPjKUuStFlD99Cr6qkkvwH8EyunLb6nqr48tsokSZsy0hh6\nVX0U+OiYahk4LDMjrHN8tkONYJ3jtB1qhO1T5zOkqqZdgyRpDLz0X5IaMfFAT7IvyVeTPJjk8Drr\nk+RPu/VfTHJF3323uM5f6ur7UpLbk1y+at3Jbvk9SU5Muc7XJfnPrpZ7kryj775bXOfvrKrx3iRP\nJ9nRrduS45nkPUnOJrl3g/Wz0jYH1Tn1ttmjxllpl4PqnHq7HElVTeyHlS9L/x34KeA5wBeAy9Zs\n8ybgY0CAq4C7+u67xXW+Grikm37juTq7+ZPAzkkey03U+TrgI8Psu5V1rtn+LcAnp3A8XwtcAdy7\nwfqpt82edc5C2xxU49TbZZ86Z6FdjvIz6R56n9sD7Af+rlbcCTw/ya6e+25ZnVV1e1U90c3eycp5\n91ttlGMyU8dzjbcC759QLRuqqs8A33yWTWahbQ6scxbaZo9juZGZOpZrTKVdjmLSgd7n9gAbbbOV\ntxbY7HsdYqXndk4Bn0hyd3dl7KT0rfPV3UfwjyV52Sb3HYfe75XkR4F9wIdXLd6q4znILLTNzZpW\n2+xj2u2ytxlvlxvyiUWblOT1rPyjuXrV4qur6tEkLwCOJbm/6wlMw+eBhap6MsmbgH8E9kyplj7e\nAvxrVa3uNc3S8dw2Zrxt2i63wKR76H1uD7DRNr1uLTAmvd4rySuAm4H9VfX4ueVV9Wj3eha4lZWP\nkVOps6q+XVVPdtMfBS5KsrPPvltZ5yrXs+Zj7RYez0FmoW32MgNt81nNSLvcjFlulxub5AA9K58A\nHgJezPe/8HjZmm3ezDO/ePps3323uM4F4EHg1WuWXww8d9X07cC+Kdb5E3z/+oIrgf/oju1MHc9u\nu+exMp558TSOZ/ceu9n4i7ypt82edU69bfaocertsk+ds9Iuh/2Z6JBLbXB7gCS/2q3/K1auNH0T\nKw3yv4C3Pdu+U6zzHcCPA3+ZBOCpWrl5zzxwa7fsQuB9VfXxKdb5i8CvJXkK+G/g+lpphbN2PAF+\nAfjnqvruqt237HgmeT8rZ1/sTPII8AfARatqnHrb7Fnn1Ntmjxqn3i571glTbpej8EpRSWqEV4pK\nUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGvF/TmWIgz5MDlkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e16022ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(entropy[correct].data.numpy(), alpha=0.5, bins=20)\n",
    "plt.hist(entropy[wrong].data.numpy(), color='orange', alpha=0.5, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(232, dtype=torch.uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(correct) + sum(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 2708]), torch.Size([2708, 1433]), torch.Size([2708]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape, features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encode_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ddfbc80693a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0madv_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ddfbc80693a7>\u001b[0m in \u001b[0;36madv_train\u001b[0;34m(nodes)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_adv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mbbb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/282_project/graph-project/gcn/dataloader.py\u001b[0m in \u001b[0;36mload_adv_data\u001b[0;34m(node, path, dataset)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                         dtype=np.dtype(str))\n\u001b[1;32m     64\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_features_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_features_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# build graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataloader import load_adv_data\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def accuracy(pred, true, idx):\n",
    "    ## Outputs accuracy on given index\n",
    "    assert(len(pred) == len(true))\n",
    "    return np.sum((pred == true)[idx]) / len(true[idx])\n",
    "\n",
    "class Counter:\n",
    "    def __init__(self, types):\n",
    "        self.values = {}\n",
    "        for t in types:\n",
    "            self.values[t] = []\n",
    "            \n",
    "    def avg(self, t):\n",
    "        return np.mean(self.values[t])\n",
    "    \n",
    "    def update(self, xs, types):\n",
    "        for i, x in enumerate(xs):\n",
    "            self.values[types[i]].append(x)\n",
    "            \n",
    "    def print_all(self):\n",
    "        for t in self.values.keys():\n",
    "            print(t, self.avg(t))\n",
    "\n",
    "def adv_train(nodes):\n",
    "    start = time.time()\n",
    "    counts = ['train_acc', 'val_acc', 'test_acc', 'adv_acc']\n",
    "    results = Counter(counts)\n",
    "    \n",
    "    for node in nodes:\n",
    "        model = GCN(nfeat=1433,\n",
    "                    nhid=16,\n",
    "                    nclass=7,\n",
    "                    dropout=0.5)\n",
    "        model.eval()\n",
    "        params = model.init_params()\n",
    "        optimizer = optimizer = optim.Adam([params], lr=2e-2, weight_decay=5e-4)\n",
    "        \n",
    "        adj, features, labels, idx_train, idx_val, idx_test = load_adv_data(node)\n",
    "        \n",
    "        bbb(adj, features, labels, optimizer, idx_train, idx_val, model, params, epochs=1000, samples=10, seed=42)\n",
    "            \n",
    "        _ = model.init_weights(params)\n",
    "        pred = model(features, adj)\n",
    "        pred = torch.argmax(pred, dim=1).numpy()\n",
    "        print(pred)\n",
    "        \n",
    "        train_acc = accuracy(pred, labels.numpy(), idx_train)\n",
    "        val_acc = accuracy(pred, labels.numpy(), idx_val)\n",
    "        test_acc = accuracy(pred, labels.numpy(), idx_test)\n",
    "        adv_acc = accuracy(pred, labels.numpy(), [node])\n",
    "        results.update([train_acc, val_acc, test_acc, adv_acc], counts)\n",
    "    \n",
    "    end = time.time()  \n",
    "    print(\"Time Taken\", end-start)\n",
    "    results.print_all()\n",
    "    return results\n",
    "\n",
    "adv_train(idx_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
