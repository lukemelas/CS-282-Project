{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "    \n",
    "def softplus(rho):\n",
    "    # convert from rho -> sigma\n",
    "    return torch.log(1 + torch.exp(rho))     \n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_weights = 0\n",
    "        \n",
    "        self.weight = None # Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.num_weights += in_features * out_features\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = None # Parameter(torch.FloatTensor(out_features))\n",
    "            self.num_weights += out_features\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "                    \n",
    "    def init_weights(self, weights):\n",
    "        self.weight = weights[:self.in_features * self.out_features].reshape(self.in_features, self.out_features)\n",
    "        if self.bias:\n",
    "            self.bias = weights[self.in_features * self.out_features:]\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.layers = [self.gc1, self.gc2]\n",
    "        self.num_weights = sum(layer.num_weights for layer in self.layers)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def kernel(self, x):\n",
    "        ## Standard RBF kernel: x -> exp(-x*x)\n",
    "        return torch.exp(-x*x)\n",
    "    \n",
    "    def init_params(self):\n",
    "        return torch.rand(self.num_weights * 2, requires_grad=True) # stored as [mu1, mu2, rho1, rho2]\n",
    "    \n",
    "    def init_weights(self, var_params):\n",
    "        ## Returns log_prob of weights, prior_prob of weights\n",
    "        w_dist = Normal(var_params[:self.num_weights], softplus(var_params[self.num_weights:]))\n",
    "        p_dist = Normal(torch.zeros(self.num_weights), torch.ones(self.num_weights) * 5)\n",
    "        weights = w_dist.rsample()\n",
    "        \n",
    "        counter = 0\n",
    "        for layer in self.layers:\n",
    "            layer.init_weights(weights[counter:counter+layer.num_weights])\n",
    "            counter += layer.num_weights\n",
    "            \n",
    "        return w_dist.log_prob(weights), p_dist.log_prob(weights)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.kernel(self.gc1(x, adj))\n",
    "        # x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "from utils2 import load_data, load_adv_data\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Added:  [2158 1136 1870] Removed:  [2444 1767  563]\n",
      "Train :: Epoch 1 | ELBO: -6.135 | LL: -4.968 \n",
      "Train :: Epoch 51 | ELBO: -3.253 | LL: -2.093 \n",
      "Train :: Epoch 101 | ELBO: -2.512 | LL: -1.346 \n",
      "Train :: Epoch 151 | ELBO: -2.064 | LL: -0.898 \n",
      "Train :: Epoch 201 | ELBO: -1.697 | LL: -0.525 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 344  183 2643] Removed:  [503 496 174]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 784 1179 2315] Removed:  [2536  565  294]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [   9 1984  333 1775] Removed:  [2396 2029  174 1851]\n",
      "Train :: Epoch 1 | ELBO: -6.965 | LL: -5.798 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.610 | LL: -1.443 \n",
      "Train :: Epoch 151 | ELBO: -2.021 | LL: -0.854 \n",
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [657] Removed:  [702]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 490 1608] Removed:  [134  74]\n",
      "Train :: Epoch 1 | ELBO: -6.971 | LL: -5.804 \n",
      "Train :: Epoch 51 | ELBO: -3.282 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.610 | LL: -1.443 \n",
      "Train :: Epoch 151 | ELBO: -2.020 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.492 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1860] Removed:  [514]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1949 1898  137  575] Removed:  [1888 2548 2391  109]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.801 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.612 | LL: -1.444 \n",
      "Train :: Epoch 151 | ELBO: -2.021 | LL: -0.853 \n",
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.494 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 116 2544 1659] Removed:  [317 523 417]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.020 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.494 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1391  840] Removed:  [314 669]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2399 1145 2614] Removed:  [192 356 519]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1696 2354] Removed:  [611  74]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1013 2521] Removed:  [411 407]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 271 1560] Removed:  [452 117]\n",
      "Train :: Epoch 1 | ELBO: -6.969 | LL: -5.802 \n",
      "Train :: Epoch 51 | ELBO: -3.282 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.611 | LL: -1.443 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [843 527] Removed:  [506 569]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1124 2388] Removed:  [2164  637]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.801 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1855] Removed:  [272]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1001 2268  729] Removed:  [ 164 1170 2509]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.442 \n",
      "Train :: Epoch 151 | ELBO: -2.020 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1657 1336 2087] Removed:  [181 766  76]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2475 1413 2516 1570 1756 1957 2472] Removed:  [ 190  280  193  192 2608   10  705]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.442 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1053 2651 1678] Removed:  [142 524 230]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2018 2362   40  385] Removed:  [ 344  638 1259  156]\n",
      "Train :: Epoch 1 | ELBO: -6.970 | LL: -5.802 \n",
      "Train :: Epoch 51 | ELBO: -3.280 | LL: -2.119 \n",
      "Train :: Epoch 101 | ELBO: -2.610 | LL: -1.443 \n",
      "Train :: Epoch 151 | ELBO: -2.020 | LL: -0.853 \n",
      "Train :: Epoch 201 | ELBO: -1.668 | LL: -0.495 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 356 2304] Removed:  [525  68]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.282 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 640 2452 1714  458  275  104  443 1480 1828 1934  524  340 1748 1459\n",
      " 1821  831] Removed:  [ 163 1676  717  565  415  551  478 1616  910 2600  188   54 2408 2642\n",
      " 2480 1687]\n",
      "Train :: Epoch 1 | ELBO: -6.969 | LL: -5.802 \n",
      "Train :: Epoch 51 | ELBO: -3.282 | LL: -2.122 \n",
      "Train :: Epoch 101 | ELBO: -2.611 | LL: -1.444 \n",
      "Train :: Epoch 151 | ELBO: -2.020 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.664 | LL: -0.491 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 323   70 1917] Removed:  [1169  430  520]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.801 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.852 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 201 1764 1689] Removed:  [1419 1425  522]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2398 2408] Removed:  [427 747]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.440 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2359 1210 1152] Removed:  [1968  747 2606]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1703] Removed:  [678]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [961 827] Removed:  [340 709]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 338 1888] Removed:  [343 163]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 358 2695] Removed:  [2187  693]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [921] Removed:  [27]\n",
      "Train :: Epoch 1 | ELBO: -6.977 | LL: -5.810 \n",
      "Train :: Epoch 51 | ELBO: -3.280 | LL: -2.119 \n",
      "Train :: Epoch 101 | ELBO: -2.606 | LL: -1.439 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.850 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 986 2400] Removed:  [777  41]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.801 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.442 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [237] Removed:  [313]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2679  849 1384] Removed:  [109 672 761]\n",
      "Train :: Epoch 1 | ELBO: -6.970 | LL: -5.802 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 380  844 1790] Removed:  [206  58 411]\n",
      "Train :: Epoch 1 | ELBO: -6.971 | LL: -5.804 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.610 | LL: -1.442 \n",
      "Train :: Epoch 151 | ELBO: -2.020 | LL: -0.853 \n",
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.494 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1037  451] Removed:  [357 606]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [ 223  875  808 2539 2372 1634] Removed:  [2165  487 2193  129 1204 1738]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2158 2159 2383 2193 1171] Removed:  [1123  183 1453 1665 1210]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1986 1459  426] Removed:  [ 543 1121  603]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.852 \n",
      "Train :: Epoch 201 | ELBO: -1.667 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1559 1793 1160] Removed:  [ 183 1123  539]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2114 1187] Removed:  [1203  779]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [134] Removed:  [540]\n",
      "Train :: Epoch 1 | ELBO: -6.970 | LL: -5.803 \n",
      "Train :: Epoch 51 | ELBO: -3.280 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.665 | LL: -0.492 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2372 2658 2482] Removed:  [  0 751  14]\n",
      "Train :: Epoch 1 | ELBO: -6.969 | LL: -5.802 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.121 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1426] Removed:  [747]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.440 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2465] Removed:  [163]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [2045 1651] Removed:  [430  74]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.801 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.018 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.492 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1992 1750 2544] Removed:  [ 552  565 1860]\n",
      "Train :: Epoch 1 | ELBO: -6.968 | LL: -5.801 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.609 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Loading cora dataset...\n",
      "Added:  [1493  423  837 2280 1793] Removed:  [456 672 507 640 591]\n",
      "Train :: Epoch 1 | ELBO: -6.967 | LL: -5.800 \n",
      "Train :: Epoch 51 | ELBO: -3.281 | LL: -2.120 \n",
      "Train :: Epoch 101 | ELBO: -2.608 | LL: -1.441 \n",
      "Train :: Epoch 151 | ELBO: -2.019 | LL: -0.851 \n",
      "Train :: Epoch 201 | ELBO: -1.666 | LL: -0.493 \n",
      "[3 5 6 ... 4 1 3]\n",
      "Time Taken 2084.835062265396\n",
      "train_acc 0.9505714285714286\n",
      "val_acc 0.7701333333333334\n",
      "test_acc 0.7721000000000002\n",
      "adv_acc 0.34\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def bbb(adj, features, labels, optimizer, idx_train, idx_val, model, params, epochs=140, samples=10, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        ## Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(params.grad)\n",
    "        \n",
    "        total_ll = 0.0\n",
    "        total_wl = 0.0\n",
    "        total_wp = 0.0\n",
    "        \n",
    "        for j in range(samples):\n",
    "                        \n",
    "            ## Init weights\n",
    "            weight_ll, weight_prior = model.init_weights(params)\n",
    "            total_wl += weight_ll.sum() / model.num_weights\n",
    "            total_wp += weight_prior.sum() / model.num_weights\n",
    "\n",
    "            ## Forward pass, compute log_likelihood\n",
    "            pred = model(features, adj)\n",
    "            model_ll = - F.nll_loss(pred[idx_train], labels[idx_train])\n",
    "            total_ll += model_ll\n",
    "\n",
    "            ## Compute loss\n",
    "            ELBO = - model_ll - (weight_prior.sum() - weight_ll.sum()) / model.num_weights\n",
    "            # Why divide by weights twice? \n",
    "\n",
    "            ## Compute gradients\n",
    "            ELBO.backward()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            total_ELBO = total_ll + total_wp - total_wl\n",
    "            print(\"Train :: Epoch {0:d} | ELBO: {1:.3f} | LL: {2:.3f} \".format(i + 1, total_ELBO / samples, total_ll / samples))\n",
    "        \n",
    "        ## Update parameters\n",
    "        # print(\"OLD\", params[:5])\n",
    "        optimizer.step()    \n",
    "        # print(\"NEW\", params[:5])\n",
    "        \n",
    "def val(adj, features, labels, idx_train, idx_val, model, params, samples=300, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    model.eval() # This could be a problem\n",
    "    \n",
    "    train_ll = 0.0\n",
    "    val_ll = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_acc = 0.0\n",
    "\n",
    "    for j in range(samples):\n",
    "        ## Forward pass\n",
    "        _ = model.init_weights(params)\n",
    "        pred = model(features, adj)\n",
    "        y_hat = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        ## collect log_likelihood statistics\n",
    "        train_ll += - F.nll_loss(pred[idx_train], labels[idx_train])\n",
    "        val_ll += - F.nll_loss(pred[idx_val], labels[idx_val])\n",
    "        \n",
    "        ## collect accuracy statistics\n",
    "        train_acc += accuracy(y_hat.numpy(), labels.numpy(), idx_train)\n",
    "        val_acc += accuracy(y_hat.detach().numpy(), labels.numpy(), idx_val)\n",
    "            \n",
    "    print(\"Val   :: Train LL: {0:.3f} | Val LL: {1:.3f} | Train Accuracy: {2:.3f} | Val Accuracy: {3:.3f} \".format(\n",
    "        train_ll / samples, val_ll / samples, train_acc / samples, val_acc / samples))\n",
    "\n",
    "def accuracy(pred, true, idx):\n",
    "    ## Outputs accuracy on given index\n",
    "    assert(len(pred) == len(true))\n",
    "    return np.sum((pred == true)[idx]) / len(true[idx])\n",
    "\n",
    "class Counter:\n",
    "    def __init__(self, types):\n",
    "        self.values = {}\n",
    "        for t in types:\n",
    "            self.values[t] = []\n",
    "            \n",
    "    def avg(self, t):\n",
    "        return np.mean(self.values[t])\n",
    "    \n",
    "    def update(self, xs, types):\n",
    "        for i, x in enumerate(xs):\n",
    "            self.values[types[i]].append(x)\n",
    "            \n",
    "    def print_all(self):\n",
    "        for t in self.values.keys():\n",
    "            print(t, self.avg(t))\n",
    "\n",
    "nodes = idx_test[:50]\n",
    "start = time.time()\n",
    "counts = ['train_acc', 'val_acc', 'test_acc', 'adv_acc']\n",
    "results = Counter(counts)\n",
    "\n",
    "for node in nodes:\n",
    "    model = GCN(nfeat=1433,\n",
    "                nhid=16,\n",
    "                nclass=7,\n",
    "                dropout=0.5)\n",
    "    params = model.init_params()\n",
    "    optimizer = optimizer = optim.Adam([params], lr=2e-2, weight_decay=5e-4)\n",
    "\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_adv_data(node)\n",
    "\n",
    "    bbb(adj, features, labels, optimizer, idx_train, idx_val, model, params, epochs=250, samples=10, seed=42)\n",
    "    model.eval()\n",
    "    _ = model.init_weights(params)\n",
    "    pred = model(features, adj)\n",
    "    pred = torch.argmax(pred, dim=1).numpy()\n",
    "    print(pred)\n",
    "\n",
    "    train_acc = accuracy(pred, labels.numpy(), idx_train)\n",
    "    val_acc = accuracy(pred, labels.numpy(), idx_val)\n",
    "    test_acc = accuracy(pred, labels.numpy(), idx_test)\n",
    "    adv_acc = accuracy(pred, labels.numpy(), [node])\n",
    "    results.update([train_acc, val_acc, test_acc, adv_acc], counts)\n",
    "\n",
    "end = time.time()  \n",
    "print(\"Time Taken\", end-start)\n",
    "results.print_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
