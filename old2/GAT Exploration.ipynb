{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from importlib import reload \n",
    "import pdb \n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions as ds\n",
    "from distributions import VonMisesFisher, HypersphericalUniform\n",
    "\n",
    "from gae.utils import load_data, mask_test_edges, preprocess_graph, get_roc_score\n",
    "from gae.layers import GraphConvolution, GraphAttentionLayer\n",
    "from gae.model import InnerProductDecoder\n",
    "from gae.sparse import SpGraphAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class obj(object):\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "               setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "args = {\n",
    "    'model': 'gcn_vae',\n",
    "    'seed': 72,\n",
    "    'epochs': 200,\n",
    "    'lr': 7.5e-3,\n",
    "    'dropout': 0.6,\n",
    "}\n",
    "args = obj(args)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we load and preprocess our data. We load the data with the loading functions from the GCN repo because this repo contains labels. We preprocess the data with functions from the GAE repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph and features\n",
    "data = torch.load('../data/cora/preprocessed_gcn_data_for_gae.pth')\n",
    "\n",
    "# Unpack data\n",
    "adj = sp.csr_matrix(data['adj_orig'])\n",
    "features = (data['feats'] != 0).float()\n",
    "labels = data['labels']\n",
    "idx_train, idx_val, idx_test = [data['idx_'+s] for s in ['train','val','test']]\n",
    "\n",
    "# Get info\n",
    "n_nodes, feat_dim = features.shape\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "# Split edges into train, val, test\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "# Normalization\n",
    "pos_weight = float(float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum())\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "# GPU\n",
    "adj_norm = adj_norm.to_dense().to(device)\n",
    "adj_label = adj_label.to(device)\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we defined a GAE with two outputs: link prediction and node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAVAE(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, c_dim, dropout, n_heads, sparse=True):\n",
    "        super(GAVAE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Construct attention layers\n",
    "        self.attentions = [SpGraphAttentionLayer(in_dim, h_dim, dropout=dropout, concat=True) \n",
    "                           for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions): # name layers\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        \n",
    "        self.attentions2 = [SpGraphAttentionLayer(h_dim * n_heads, h_dim, dropout=dropout, concat=True) \n",
    "                           for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions2): # name layers\n",
    "            self.add_module('attention2_{}'.format(i), attention)\n",
    "\n",
    "        # Output encoder layer\n",
    "        self.mu_att = GraphAttentionLayer(h_dim * n_heads, h_dim, dropout=dropout, concat=False)\n",
    "        self.lv_att = GraphAttentionLayer(h_dim * n_heads,     1, dropout=dropout, concat=False)\n",
    "        \n",
    "        # Decoder layer\n",
    "        self.linear = nn.Linear(h_dim, c_dim)\n",
    "        \n",
    "    def encode(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions2], dim=1) # added\n",
    "        x = F.dropout(x, self.dropout, training=self.training) # added\n",
    "        mu = self.mu_att(x, adj) \n",
    "        lv = self.lv_att(x, adj)\n",
    "        return mu, lv\n",
    "\n",
    "    def reparameterize(self, mu, lv):\n",
    "        q = ds.Normal(mu, F.softplus(lv) + 1)\n",
    "        p = ds.Normal(torch.zeros_like(mu), torch.ones_like(lv))\n",
    "        #mu, lv = mu.cpu(), lv.cpu()\n",
    "        #q = VonMisesFisher(mu / mu.norm(dim=-1, keepdim=True), F.softplus(lv) + 1)\n",
    "        #p = HypersphericalUniform(7)\n",
    "        return q, p\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        mu, lv = self.encode(x, adj) # get mean and log standard deviation\n",
    "        q, p = self.reparameterize(mu, lv) # get variational dist and prior\n",
    "        if self.training: # sample latent variable\n",
    "            z = q.rsample().to(device)\n",
    "        else: # use mean\n",
    "            z = q.loc.to(device)\n",
    "        o = self.linear(z) # decode\n",
    "        return o, q, p, z, mu, lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    '''Accuracy of prediction (max of prediction)'''\n",
    "    preds = y_hat.max(1)[1].type_as(y)\n",
    "    acc = preds.eq(y).float().sum() / len(y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-322004fadc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Manual seed\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Init model \n",
    "model = GAVAE(in_dim=feat_dim, \n",
    "              h_dim=8,\n",
    "              c_dim=labels.max().item() + 1,\n",
    "              dropout=args.dropout,\n",
    "              n_heads=8).to(device)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# Train\n",
    "val_every = 10\n",
    "hidden_emb = None\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass (output, q(z), p(z), mean, log standard deviation)\n",
    "    out, q, p, z, mu, lv = model(features, adj_norm)\n",
    "    \n",
    "    # Loss\n",
    "    log_likelihood = F.cross_entropy(out[idx_train], labels[idx_train]).mean()\n",
    "    kl_divergence = torch.distributions.kl.kl_divergence(q,p).mean().to(device)\n",
    "    loss = log_likelihood + 0 * kl_divergence\n",
    "    \n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Statistics\n",
    "    acc_val_tmode = accuracy(out[idx_val], labels[idx_val])\n",
    "    print_string = ('Epoch [{:4d}]   '.format(epoch + 1) + \n",
    "                    'Train loss = {:.3f}   '.format(loss.item()) + \n",
    "                    'Val_tmode acc = {:.3f}   '.format(acc_val_tmode.item()))\n",
    "    \n",
    "    # Val\n",
    "    if epoch % val_every == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out, q, p, z, mu, lv = model(features, adj_norm)\n",
    "            val_acc = accuracy(out[idx_test], labels[idx_test])\n",
    "        print_string += ('Val acc = {:.3f}   '.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
